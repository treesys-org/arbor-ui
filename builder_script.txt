

import os
import json
import datetime
import re
import uuid
import shutil
import time

# ==========================================
# üå≥ ARBOR KNOWLEDGE BUILDER (INCREMENTAL)
# ==========================================
# This script compiles the 'content/' directory into the static JSON API.
# 
# NEW FEATURES:
# - Incremental Builds: Skips processing files that haven't changed.
# - Orphan Cleanup: Deletes JSON shards for removed content files.
# - Cache: Uses .arbor_build_cache.json to track file states.
# ==========================================

# --- CONFIGURATION ---
UNIVERSE_ID = "arbor-standard" 
UNIVERSE_NAME = "Arbor Standard Curriculum"
NAMESPACE = "org.arbor.knowledge"

ROOT_DIR = "./content" 
DATA_ROOT_DIR = "./data"
API_DIR = os.path.join(DATA_ROOT_DIR, "nodes")
CONTENT_API_DIR = os.path.join(DATA_ROOT_DIR, "content") 
SEARCH_DIR = os.path.join(DATA_ROOT_DIR, "search") 
OUTPUT_FILE = os.path.join(DATA_ROOT_DIR, "data.json") 
CACHE_FILE = ".arbor_build_cache.json"

# Standardize on Markdown as the sole format for clarity.
ALLOWED_EXTENSIONS = {'.md'}

# Global trackers for incremental logic
BUILD_CACHE = {}
GENERATED_CONTENT_PATHS = set()
STATS = {"cached": 0, "processed": 0, "deleted": 0}

# --- UTILS ---

def load_cache():
    global BUILD_CACHE
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                BUILD_CACHE = json.load(f)
        except:
            BUILD_CACHE = {}

def save_cache():
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(BUILD_CACHE, f, indent=0)
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Could not save build cache: {e}")

def parse_frontmatter(content):
    """Parses standard Jekyll/Hugo style frontmatter and normalizes keys."""
    meta = {}
    clean_content = content
    match = re.match(r'^---\n(.*?)\n---\n(.*)', content, re.DOTALL)
    if match:
        frontmatter_str, clean_content = match.groups()
        clean_content = clean_content.strip()
        
        # Map localized keys to internal schema
        key_map = {
            'titulo': 'name', 'title': 'name',
            'icono': 'icon', 'icon': 'icon',
            'descripcion': 'description', 'description': 'description',
            'orden': 'order', 'order': 'order',
            'discusion': 'discussionUrl', 'discussion': 'discussionUrl',
            'tipo': 'type', 'type': 'type'
        }

        for line in frontmatter_str.split('\n'):
            if ':' in line:
                key, val = line.split(':', 1)
                raw_key = key.strip().lower()
                val = val.strip().strip('"\'')
                if raw_key in key_map: meta[key_map[raw_key]] = val
                else: meta[raw_key] = val
                    
    return meta, clean_content

def parse_arbor_format(content):
    """Parses the custom Arbor @tag syntax."""
    meta = {}
    content_lines = []
    lines = content.split('\n')
    parsing_metadata = True

    for line in lines:
        stripped = line.strip()
        if parsing_metadata and not stripped: continue

        if parsing_metadata and stripped.startswith('@'):
            if stripped.lower() == '@exam':
                meta['type'] = 'exam'
                continue

            if ':' in stripped:
                key_part, val_part = stripped.split(':', 1)
                raw_key = key_part[1:].strip().lower()
                val = val_part.strip()
                
                key_map = {
                    'titulo': 'name', 'title': 'name',
                    'icono': 'icon', 'icon': 'icon',
                    'descripcion': 'description', 'description': 'description',
                    'orden': 'order', 'order': 'order',
                    'discusion': 'discussionUrl', 'discussion': 'discussionUrl',
                    'tipo': 'type', 'type': 'type'
                }
                content_tags = ['image', 'img', 'video', 'audio', 'quiz', 'section', 'h1', 'h2', 'option', 'correct']
                
                if raw_key in content_tags:
                     parsing_metadata = False
                     content_lines.append(line)
                elif raw_key in key_map:
                     meta[key_map[raw_key]] = val
                else:
                     meta[raw_key] = val
            else:
                 parsing_metadata = False
                 content_lines.append(line)
        else:
            if parsing_metadata: parsing_metadata = False
            content_lines.append(line)
            
    return meta, '\n'.join(content_lines).strip()

def get_or_create_persistent_id(folder_path):
    """Ensures every folder has a UUID so links don't break if folders are renamed."""
    meta_path = os.path.join(folder_path, "meta.json")
    meta_data = {}
    
    if os.path.exists(meta_path):
        try:
            with open(meta_path, 'r', encoding='utf-8-sig') as f:
                meta_data = json.load(f)
        except: meta_data = {}

    raw_uuid = meta_data.get("uuid")
    if not raw_uuid:
        raw_uuid = str(uuid.uuid4())
        meta_data["uuid"] = raw_uuid
        try:
            with open(meta_path, 'w', encoding='utf-8') as f:
                json.dump(meta_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"‚ùå Error saving UUID to {folder_path}: {e}")

    return meta_data, f"{NAMESPACE}::{raw_uuid}"

def tokenize(text):
    if not text: return []
    text = text.lower()
    words = re.findall(r'\w+', text)
    return [w for w in words if len(w) > 2]

def add_to_search_shards(node, shards_dict):
    text_to_index = f"{node.get('name', '')} {node.get('description', '')}"
    words = tokenize(text_to_index)
    target_shards = set()
    
    for word in words:
        if len(word) >= 2:
            prefix = word[:2]
            target_shards.add(prefix)
            
    for prefix in target_shards:
        if prefix not in shards_dict: shards_dict[prefix] = []
        search_entry = {
            "id": node.get("id"),
            "n": node.get("name"),
            "t": node.get("type"),
            "i": node.get("icon"),
            "d": node.get("description"),
            "p": node.get("path"),
            "l": node.get("lang"),
            "c": node.get("isCertifiable", False)
        }
        shards_dict[prefix].append(search_entry)

def build_tree_recursive(path, lang_folder, parent_id=None, node_collector=None, breadcrumb_path="", relative_slug_path=""):
    """Recursively walks the directory structure to build the node tree."""
    if node_collector is None: node_collector = []

    name = os.path.basename(path)
    name_slug = re.sub(r'[^a-zA-Z0-9]+', '-', os.path.splitext(name)[0]).lower().strip('-')

    current_slug_path = name_slug
    if relative_slug_path:
        current_slug_path = f"{relative_slug_path}/{name_slug}"

    clean_source_path = os.path.relpath(path, ".").replace("\\", "/")

    # --- FOLDER (BRANCH) ---
    if os.path.isdir(path):
        meta, node_id = get_or_create_persistent_id(path)
        display_name = meta.get("name", name.replace('_', ' '))
        
        node = {
            "id": node_id, 
            "name": display_name,
            "parentId": parent_id, 
            "icon": meta.get("icon", "üìÅ"), 
            "description": meta.get("description", ""),
            "expanded": False, 
            "status": "available",
            "type": "branch",
            "namespace": NAMESPACE,
            "apiPath": f"{lang_folder.lower()}/{current_slug_path}",
            "totalLeaves": 0,
            "leafIds": [],
            "sourcePath": clean_source_path,
            "isCertifiable": False
        }
        
        current_breadcrumb = f"{breadcrumb_path} / {display_name}" if breadcrumb_path else display_name
        node['path'] = current_breadcrumb

        child_items = sorted([item for item in os.listdir(path) if not item.startswith('.') and item != 'meta.json']) if os.path.exists(path) else []
        
        if child_items:
            node["hasUnloadedChildren"] = True
            children_for_api_file = []
            
            for item in child_items:
                item_path = os.path.join(path, item)
                child_node = build_tree_recursive(item_path, lang_folder, node_id, node_collector, current_breadcrumb, current_slug_path)
                if child_node:
                    children_for_api_file.append(child_node)
                    if child_node.get('type') in ['leaf', 'exam']:
                        node["totalLeaves"] += 1
                        node["leafIds"].append(child_node['id'])
                    else:
                        node["totalLeaves"] += child_node.get('totalLeaves', 0)
                        if 'leafIds' in child_node: node["leafIds"].extend(child_node['leafIds'])
            
            children_for_api_file.sort(key=lambda x: (int(x.get('order', 999)), x.get('name', '')))
            if any(child.get('type') == 'exam' for child in children_for_api_file): node['isCertifiable'] = True

            # We assume Structure JSONs are cheap to write, so we write them every time to ensure consistency
            api_relative_path = os.path.join(lang_folder.lower(), *current_slug_path.split('/')) + ".json"
            api_filepath = os.path.join(API_DIR, api_relative_path)
            
            if api_filepath:
                os.makedirs(os.path.dirname(api_filepath), exist_ok=True)
                with open(api_filepath, 'w', encoding='utf-8') as f:
                    json.dump(children_for_api_file, f, ensure_ascii=False, separators=(',', ':'))

    # --- FILE (LEAF) ---
    else: 
        ext = os.path.splitext(path)[1].lower()
        if ext in ALLOWED_EXTENSIONS:
            node_id = f"{parent_id}__{name_slug}"
            
            # Paths
            content_rel_path = f"{lang_folder.lower()}/{current_slug_path}.json"
            content_full_path = os.path.join(CONTENT_API_DIR, content_rel_path)
            
            # Track this path so we don't delete it later
            GENERATED_CONTENT_PATHS.add(os.path.abspath(content_full_path))

            # INCREMENTAL CHECK
            mtime = os.path.getmtime(path)
            cached_data = BUILD_CACHE.get(path)
            
            needs_processing = True
            
            # If cache matches and output file exists, skip reading source
            if cached_data and cached_data.get('mtime') == mtime and os.path.exists(content_full_path):
                needs_processing = False
                meta = cached_data.get('meta', {})
                STATS["cached"] += 1
            
            node = {
                "id": node_id,
                "parentId": parent_id,
                "type": "leaf",
                "namespace": NAMESPACE,
                "name": os.path.splitext(name)[0].replace('_', ' '),
                "icon": "üìÑ",
                "sourcePath": clean_source_path,
                "contentPath": content_rel_path
            }

            if needs_processing:
                try:
                    with open(path, 'r', encoding='utf-8-sig') as f:
                        raw_content = f.read()
                    
                    if ext == '.md' and raw_content.startswith('---'):
                         meta, content = parse_frontmatter(raw_content)
                    else:
                         meta, content = parse_arbor_format(raw_content)
                    
                    # Write Content Shard
                    os.makedirs(os.path.dirname(content_full_path), exist_ok=True)
                    with open(content_full_path, 'w', encoding='utf-8') as f:
                         json.dump({"content": content}, f, ensure_ascii=False)
                    
                    # Update Cache
                    BUILD_CACHE[path] = {'mtime': mtime, 'meta': meta}
                    STATS["processed"] += 1

                except Exception as e:
                    print(f"‚ùå Error reading {path}: {e}")
                    node["description"] = f"Error: {e}"
                    meta = {}
            
            # Apply metadata (either from fresh parse or cache)
            node.update(meta)
            node['path'] = f"{breadcrumb_path} / {node['name']}"

            if node.get('type') == 'exam' and node.get('icon') == 'üìÑ':
                node['icon'] = '‚öîÔ∏è'
                
        else:
            return None

    # Search indexing relies on metadata, which is populated in 'node' regardless of cache hit/miss
    search_node = {
        "id": node.get("id"), "name": node.get("name"), "type": node.get("type"),
        "icon": node.get("icon"), "description": node.get("description"), 
        "lang": lang_folder.upper(), "path": node.get("path"), 
        "isCertifiable": node.get("isCertifiable", False)
    }
    node_collector.append(search_node)
    
    if 'children' in node: del node['children']
    return node

def clean_orphans():
    """Deletes JSON content files that no longer have a source Markdown file."""
    if not os.path.exists(CONTENT_API_DIR): return
    
    for root, dirs, files in os.walk(CONTENT_API_DIR):
        for file in files:
            full_path = os.path.abspath(os.path.join(root, file))
            if full_path not in GENERATED_CONTENT_PATHS:
                try:
                    os.remove(full_path)
                    STATS["deleted"] += 1
                except: pass
        
        # Cleanup empty dirs
        if not os.listdir(root) and root != os.path.abspath(CONTENT_API_DIR):
            try: os.rmdir(root)
            except: pass

def main():
    start_time = time.time()
    print(f"\nüå≥ Arbor Knowledge Builder (Incremental Rsync-Mode)")
    print(f"==================================================")
    
    load_cache()

    if not os.path.exists(ROOT_DIR):
        print(f"‚ùå Error: Content directory '{ROOT_DIR}' not found.")
        return

    # Don't wipe CONTENT_API_DIR to allow incremental updates.
    # Wiping API_DIR (structure) and SEARCH_DIR is fine as they are fast.
    if os.path.exists(SEARCH_DIR): shutil.rmtree(SEARCH_DIR)
    if os.path.exists(API_DIR): shutil.rmtree(API_DIR)
    
    os.makedirs(DATA_ROOT_DIR, exist_ok=True)
    os.makedirs(SEARCH_DIR, exist_ok=True)
    os.makedirs(API_DIR, exist_ok=True)
    os.makedirs(CONTENT_API_DIR, exist_ok=True)

    full_data = {
        "generatedAt": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "universeId": UNIVERSE_ID,
        "universeName": UNIVERSE_NAME,
        "languages": {}
    }

    language_folders = sorted([d for d in os.listdir(ROOT_DIR) if os.path.isdir(os.path.join(ROOT_DIR, d)) and not d.startswith('.')])
    
    for lang_folder in language_folders:
        print(f"üî® Processing: {lang_folder.upper()}")
        lang_path = os.path.join(ROOT_DIR, lang_folder)
        
        root_id = f"{UNIVERSE_ID}-{lang_folder.lower()}-root"
        root_name = f"Arbor {lang_folder.upper()}"
        
        root_node = { 
            "id": root_id, "name": root_name, "parentId": None, 
            "icon": "üå≥", "expanded": True, "status": "available", 
            "type": "root", "description": f"{lang_folder.upper()} Curriculum", 
            "path": root_name 
        }
        
        lang_search_nodes = []
        root_children_nodes = []
        top_level_items = sorted([item for item in os.listdir(lang_path) if not item.startswith('.') and item != 'meta.json'])

        for item in top_level_items:
            item_path = os.path.join(lang_path, item)
            branch_search_nodes = []
            top_node = build_tree_recursive(item_path, lang_folder, root_node['id'], branch_search_nodes, root_name)
            
            if top_node:
                root_children_nodes.append(top_node)
                lang_search_nodes.extend(branch_search_nodes)

        # Generate Search Shards (Always rebuild search to ensure consistency)
        lang_shards = {}
        for node in lang_search_nodes:
            add_to_search_shards(node, lang_shards)
        
        lang_search_dir = os.path.join(SEARCH_DIR, lang_folder.upper())
        os.makedirs(lang_search_dir, exist_ok=True)
        
        for prefix, items in lang_shards.items():
            safe_prefix = re.sub(r'[^a-z0-9]', '', prefix)
            if safe_prefix:
                shard_subdir = os.path.join(lang_search_dir, safe_prefix[0])
                os.makedirs(shard_subdir, exist_ok=True)
                with open(os.path.join(shard_subdir, f"{safe_prefix}.json"), 'w', encoding='utf-8') as f:
                    json.dump(items, f, ensure_ascii=False, separators=(',', ':'))

        root_children_nodes.sort(key=lambda x: (int(x.get('order', 999)), x.get('name', '')))
        root_node['children'] = root_children_nodes
        full_data["languages"][lang_folder.upper()] = root_node

    # Write Main Data
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f: 
        json.dump(full_data, f, ensure_ascii=False, separators=(',', ':'))
    
    # Cleanup & Save Cache
    clean_orphans()
    save_cache()

    elapsed = time.time() - start_time
    print(f"==================================================")
    print(f"‚úÖ Build Complete in {elapsed:.2f}s")
    print(f"   - Cached:    {STATS['cached']} files (Skipped)")
    print(f"   - Processed: {STATS['processed']} files (Updated)")
    print(f"   - Deleted:   {STATS['deleted']} orphans")
    print(f"==================================================\n")

if __name__ == "__main__":
    main()
